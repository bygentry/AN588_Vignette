---
title: "Linear and Quadratic Discriminant Analysis"
author: "Gentry Miller and Alexandra Walsh"
format: html
editor: visual
---

## working notes - will be deleted

-   module7

-   bootstrap data to make predictions while representing all genus levels in training set for LDA? normal? outliers?

-   box plots representing means by genera? 63 genera tho... example w/ single genus and instructions for how it could be done with all the other genera

-   strapping: filter genus -\> mu = mean for each stat -\> rpois w/ n = 10000 and mu ??

-   rework: challenge 1 -\> use simulated data to manually create a new axis (reduce dimensionality) with 2 variables

    -   $\frac{(\mu_1 - \mu_2)^2}{s_1^2 + s_2^2}$

    -   $d = \mu_1 - \mu_2$

    -   $\frac{d^2}{s_1^2 + s_2^2}$

    -   d = "distance" between means

    -   by minimizing variation ("scatter", $s^2$) within each category, the denominator is reduced creating a larger ratio of distance over scatter

-   challenge 2 -\> do the same with more than 2 variables

    -   instead of measuring distance between means, find point central to all data (C)

    -   measure disances between a point central to each category and the point C

    -   maximize distance between each category and C while minimizing the scatter for each category

    -   .$$\frac{d_1^2 + d_2^2 + ... +d_n^2}{s_1^2 + s_2^2 + ... + s_n^2}$$

    -   

-   challenge 3 -\> use lda function with Kamilar and Cooper data (if possible)

## Preliminaries

### Packages

```{r}
#| label: Loading Packages

repos = "http://cran.us.r-project.org"

if(!require("tidyverse")){ install.packages("tidyverse", repos = repos) } 
if(!require("MASS")){ install.packages("MASS", repos = repos) }
if(!require("ggplot2")){ install.packages("ggplot2", repos = repos) }
if(!require("klaR")){ install.packages("klar", repos = repos) }
if(!require("curl")){ install.packages("curl", repos = repos) }
if(!require("psych")){ install.packages("psych", repos = repos) }

library(tidyverse)
library(MASS)
library(ggplot2)
library(klaR)
library(curl)
library(psych)
```

### Loading Data

```{r}
#| label: Loading Data

# store data from gibbon-femurs.csv in holder f
f <- curl("https://raw.githubusercontent.com/bygentry/AN588_Vignette/refs/heads/main/KamilarAndCooperData.csv")

# convert data in f to a data frame and store in holder d
d <- read.csv(f, sep = ",", stringsAsFactors = TRUE)
```

### Cleaning Data

First, let's create a new `data.frame` to hold our clean data. Then, we can clean it up.

```{r}
#| label: Cleaning Data I  

# creates a new data.frame "tidy.data" as a copy of "d" 
tidy.data <- d  
# using the str() command we can see that Brain_Size_Species_Mean has at least one NA value 
str(tidy.data[5]) 
tidy.data[2, 5]
```

We can check the other columns in the same way and see that all of them contain `NA` values somewhere. Luckily, the base R package `{stats}` comes with some functions that makes removing these easy - we'll be using `complete.cases()`:

```{r}
#| label: Cleaning Data II  
tidy.data <- tidy.data[complete.cases(tidy.data[, c(5, 8:10)]),] 
str(tidy.data)
```

Looks like 42 rows among those columns had `NA` values. Let's verify that we're `NA` free now.

```{r}
#| label: Cleaning Data III  
# iterates over each row in tidy.data columns 5, 8, 9, and 10 and if an NA is found, prints the index of that NA 
for(i in 1:nrow(tidy.data)){
  for(j in c(5, 8, 9, 10)){
    if(is.na(tidy.data[i, j])){
      print(paste0("NA at ", i,", ", j))
      }
    }
  }
```

Cool, we're `NA` free. Thankfully, there is a much more succinct way to do this from the start:

```{r}
#| label: Cleaning Data IV  
tidy.data <- d[complete.cases(d[ , c(5, 8:10)]), ]
```

## Introduction

### What is Discriminant Analysis?

Discriminant analysis is a type of statistical analysis used to predict the probability of belonging to a given class or category based on one or more predictor variables. We will be covering two different types of discriminant analyses: linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA).

Linear discriminant analysis is used to find a linear combination of features that separates different classes in a data set. LDA works by calculating the means and covariance matrices for each data class, assuming a common covarience matrix between all classes. When effectively utilized, LDA finds the linear combination of variables that provide the best possible separation between the groups.

Quadratic discriminant analysis is similar to LDA, but assumes that that each class has its own unique covariance matrix. QDA can be thought of as an extension to LDA, as it is still classifying observations from a set of predictor variables. Since each class has its own, unique covariance matrix, QDA allows for complex, non-linear modeling.

#### Properties and Assumptions of LDA

-   LDAs operate by projecting a feature space (a data set of n dimensions) onto a smaller space "k", where k \<= n-1 without losing class information.

-   An LDA model comprises the statistical properties that are calculated for the data in each class - where there are multiple variables, these properties are calculated over the multivariate Gaussian (normal) distribution -the Gaussian distribution is a probability distribution that is symmetric about the mean, resulting in a "bell curve" shape multivariates are:

#### Means

-   Covariance matrix (measures how each variable or feature relates to others within the same class). The model assumes the following:

1.  the input dataset has a normal distribution

2.  the dataset is linearly separable, meaning LDA can draw a decision boundary that separates the data points

3.  each class has the same covariance matrix

-   These assumptions carry the limitation that LDA may not perform well in high-dimensional feature spaces

#### Linear transformations are analyzed using eigenvectors and eigenvalues

-   Eigenvectors provide the "direction" within the scatterplot
-   During dimensionality reduction, the eigenvectors are calculated from the data set and collected in two scatter-matrices:
    -   between-class scatter matrix representing info about the spread within each class
    -   within-class scatter matrix representing how classes are spread between themselves
-   Eigenvalues denote the importance of the directional data
    -   A high eigenvalue means the associated eigenvector is more critical

## Implementing Discriminant Analysis

### Getting Started

Let's start by simulating some data. Linear Discriminant Analysis works best with normal, continuous variables with low colinearity. Considering this, let's imagine a study where a geneticist wants to classify a monkey into one of two classes based on the number of transcriptions of certain genes:

```{r}
#| label: Simulating Data

# First, let's set a seed for reproducibility,
set.seed(647)

# then set n as the number of observations to simulate.
n <- 1000

# Now we can simulate the transcription frequencies.
# Let's assume that the classification is set up as such:
# Class A: higher expression of genes X and Z
# Class B: higher expression of gene Y

# for our purposes, let 0 = class B and 1 = class A
monkey.class <- sample(c(0, 1), size = n, replace = TRUE)

# now the gene expressions

gene_x <- rnorm(n, mean = 5 - (2 * monkey.class), sd = 1) 
# this will produce a distribution that is higher in class B individuals
gene_y <- rnorm(n, mean = 5 + (2 * monkey.class), sd = 1)
# this will produce a distribution that has higher class A frequency
gene_z <- rnorm(n, mean = 5 - monkey.class, sd = 1)
# this will produce a distribution that has slightly more class B individuals

# Now, let's store this simulated data
lda_data <- data.frame(
  gene_x = gene_x,
  gene_y = gene_y,
  gene_z = gene_z,
  class = factor(monkey.class, labels = c("B", "A"))
)

# and lastly, check the first few rows to make sure everything is right
head(lda_data)
```

#### Data Partitioning

In order to predict the monkey's class, we'll need to create a training data set through random sampling.

> Note: Linear discriminant analysis requires at least one data point for each class in the training set, so if there are any classes not represented, this will fail.

We'll use 66% of the set for training and the other 34% for testing

```{r}
ind <- sample(2, nrow(lda_data), 
              replace = TRUE,
              prob = c(0.66, 0.34))

training <- lda_data[ind == 1, ]
testing <- lda_data[ind == 2, ]
```

### Challenge 1 - Linear Discriminant Analysis with 2 Variables

-   rework: challenge 1 -\> use simulated data to manually create a new axis (reduce dimensionality) with 2 variables

    -   $\frac{(\mu_1 - \mu_2)^2}{s_1^2 + s_2^2}$

    -   $d = \mu_1 - \mu_2$

    -   $\frac{d^2}{s_1^2 + s_2^2}$

    -   d = "distance" between means

    -   by minimizing variation ("scatter", $s^2$) within each category, the denominator is reduced creating a larger ratio of distance over scatter

Linear discriminant analysis reduces the number of dimensions by creating a new axis while simultaneously considering two criteria:

1.  On the new axis, we want to **maximize** the distance between the means of each variable
2.  On the new axis, we want to **minimize** the spread ( $s^2$ )in each class

We can accomplish this by expressing the rate:

$$
\frac{(\mu_1 - \mu_2)^2}{s^2_1 + s^2_2}
$$

```{r}
# For an example where we only consider two genes, lets start by calculating the values we need. The means are easy, 

mu1 <- colMeans(lda_data[lda_data$class == "A", 1:2])
mu2 <- colMeans(lda_data[lda_data$class == "B", 1:2])

# but things get a little tricky when it comes to spread. 
# Because LDA assumes that the data all share the same covariance matrix (a.k.a pooled covariance matrix),  we can't just use the standard deviations of the raw data.

# The covariance matrix is computed internally by lda(), but you can compute it manually like this:
S1 <- cov(lda_data[lda_data$class == "A", 1:2])
S2 <- cov(lda_data[lda_data$class == "B", 1:2])
n1 <- sum(lda_data$class == "A")
n2 <- sum(lda_data$class == "B")
cov_pooled <- ((n1 - 1)*S1 + (n2 - 1)*S2) / (n1 + n2 - 2)

cov_pooled

# To calculate the eigen values that serve as coefficients for the linear boundary, we have to use the inverse of the covariance matrix

cov_inv <- solve(cov_pooled)

w <- as.vector(cov_inv %*% (mu1 - mu2))
b <- -0.5 * t(mu1 + mu2) %*% cov_inv %*% (mu1 - mu2)

# Create a grid to visualize the boundary
x_vals <- seq(min(lda_data$gene_x), max(lda_data$gene_x), length.out = 100)
y_vals <- (-w[1] * x_vals - b) / w[2]  # Solving for y
boundary <- data.frame(gene_x = x_vals, gene_y = as.vector(y_vals)
)
```

```{r}
# Plot data and boundary
ggplot(lda_data, aes(x = gene_x, y = gene_y, color = class)) +
  geom_point()  +
  geom_line(data = boundary, aes(x = x_vals, y = y_vals), color = "black", linetype = "dashed") +
  labs(title = "LDA Boundary for gene_x vs gene_y") + 
  theme_bw() 
```

```{r}

```

## Quadratic Discriminant Analysis

First, we must set seed for reproducibility.

```{r}
set.seed(482)
```

Now, we nee to split the data into training and test. We'll use 70% for training and 30% for testing.

```{r}
train_index <- sample(1:nrow(tidy.data), 0.7 * nrow(tidy.data)) train_data <- tidy.data[train_index, ] test_data <- tidy.data[-train_index, ]
```

Now we will fit the model for all for features.

```{r}
qda_model <- qda(Species ~ ., data = train_data) 
```

To view the model:

```{r}
print(qda_model)
```

We first want to make prediction, based on the test set.

```{r}
qda_pred <- predict(qda_model, test_data) 
```

Next, display predicted classes.

```{r}
head(qda_predclass)
```

Create a confusion matrix and a print confusion matrix.

A confusion matrix evaluates the performance of classification models. It provides a detailed breakdown of possible prediction outcomes. A

confusion matrix can be made as a table, to compare the positives and negatives of the outcomes.

A print confusion matrix refers to the act of generating the confusion matrix table.

```{r}
conf_matrix <- table(Predicted = qda_pred$class, Actual = test_data$Species)print(conf_matrix)
```

Finally, check for accuracy.

```{r}
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix) cat("Accuracy:", round(accuracy * 100, 2), "%\n")
```
