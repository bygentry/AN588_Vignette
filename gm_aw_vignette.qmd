---
title: "Linear and Quadratic Discriminant Analysis"
author: "Gentry Miller and Alexandra Walsh"
format: html
editor: visual
---

## working notes - will be deleted 

-   see equation 20.2 in book of R

-   linear regression: predicting value of response var y based on observed relationship with predictor variable x. used to construct linear models of a relationship between x and y

-   residual: difference of actual and predicted values in response variable

-   bootstrap data to make predictions while representing all genus levels in training set for LDA? normal? outliers?

-   box plots representing means by genera? 63 genera tho... example w/ single genus and instructions for how it could be done with all the other genera

-   strapping: filter genus -\> mu = mean for each stat -\> rpois w/ n = 10000 and mu ??

## Preliminaries

```{r}
#| label: Loading Packages

repos = "http://cran.us.r-project.org"

if(!require("tidyverse")){ install.packages("tidyverse", repos = repos) } 
if(!require("MASS")){ install.packages("MASS", repos = repos) }
if(!require("ggplot2")){ install.packages("ggplot2", repos = repos) }
if(!require("klaR")){ install.packages("klar", repos = repos) }
if(!require("curl")){ install.packages("curl", repos = repos) }
if(!require("psych")){ install.packages("psych", repos = repos) }

library(tidyverse)
library(MASS)
library(ggplot2)
library(klaR)
library(curl)
library(psych)
```

```{r}
#| label: Loading Data

# store data from gibbon-femurs.csv in holder f
f <- curl("https://raw.githubusercontent.com/bygentry/AN588_Vignette/refs/heads/main/KamilarAndCooperData.csv")

# convert data in f to a data frame and store in holder d
d <- read.csv(f, sep = ",", stringsAsFactors = TRUE)
```

## Introduction

### What is Discriminant Analysis? 

Discriminant analysis is a type of statistical analysis used to predict the probability of belonging to a given class or category based on one or more predictor variables. We will be covering two different types of discriminant analyses: linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA).

Linear discriminant analysis is used to find a linear combination of features that separates different classes in a data set. LDA works by calculating the means and covariance matrices for each data class, assuming a common covarience matrix between all classes. When effectively utilized, LDA finds the linear combination of variables that provide the best possible separation between the groups.

Quadratic discriminant analysis is similar to LDA, but assumes that that each class has its own unique covariance matrix. QDA can be thought of as an extension to LDA, as it is still classifying observations from a set of predictor variables. Since each class has its own, unique covariance matrix, QDA allows for complex, non-linear modeling.

#### Properties and Assumptions of LDA

-   LDAs operate by projecting a feature space (a data set of n dimensions) onto a smaller space "k", where k \<= n-1 without losing class information.

-   An LDA model comprises the statistical properties that are calculated for the data in each class - where there are multiple variables, these properties are calculated over the multivariate Gaussian (normal) distribution -the Gaussian distribution is a probability distribution that is symmetric about the mean, resulting in a "bell curve" shape multivariates are:

#### Means

-   Covariance matrix (measures how each variable or feature relates to others within the same class). The model assumes the following:

1.  the input dataset has a normal distribution

2.  the dataset is linearly separable, meaning LDA can draw a decision boundary that separates the data points

3.  each class has the same covariance matrix

-   These assumptions carry the limitation that LDA may not perform well in high-dimensional feature spaces

#### Linear transformations are analyzed using eigenvectors and eigenvalues

-   Eigenvectors provide the "direction" within the scatterplot
-   During dimensionality reduction, the eigenvectors are calculated from the data set and collected in two scatter-matrices:
    -   between-class scatter matrix representing info about the spread within each class
    -   within-class scatter matrix representing how classes are spread between themselves
-   Eigenvalues denote the importance of the directional data
    -   A high eigenvalue means the associated eigenvector is more critical

## Implementing Discriminant Analysis

### Getting Started

The first step is to identify your predictor variables. Ideally, these are continuous variables that are normally distributed. In the Kamilar and Cooper data,

Let's start by exploring our predictor variables by themselves. Using the Kamilar and Cooper data, we should start be cleaning up our data set. For our purposes, we need our data to not contain any `NA` values in the columns we want to use as predictors.

```{r}
str(d)
```

So, we see that our data have 213 observations of 44 variables, but we're only going to use 4 of those variables in our analysis:

```{r}
colnames(d[ , c(5, 8:10)])
```

Using these four statistics, we can use LDA to make predictions about classifying these primates by species.

#### Cleaning Our Data

First, let's create a new `data.frame` to hold our clean data. Then, we can clean it up.

```{r}
#| label: Cleaning Our Data I

# creates a new data.frame "tidy.data" as a copy of "d"
tidy.data <- d

# using the str() command we can see that Brain_Size_Species_Mean has at least one NA value
str(tidy.data[5])
tidy.data[2, 5]
```

We can check the other columns in the same way and see that all of them contain `NA` values somewhere. Luckily, the base R package `{stats}` comes with some functions that makes removing these easy - we'll be using `complete.cases()`:

```{r}
#| label: Cleaning Our Data II

tidy.data <- tidy.data[complete.cases(tidy.data[ , c(5, 8:10)]), ]
str(tidy.data)
```

Looks like 42 rows among those columns had `NA` values. Let's verify that we're `NA` free now.

```{r}
#| label: Cleaning Our Data III

# iterates over each row in tidy.data columns 5, 8, 9, and 10 and if an NA is found, prints the index of that NA
for(i in 1:nrow(tidy.data)){
  for(j in c(5, 8, 9, 10))
    if(is.na(tidy.data[i, j])){
      print(paste0("NA at ", i,", ", j))
  }
}
```

Cool, we're `NA` free. Thankfully, there is a much more succinct way to do this from the start:

```{r}
#| label: Cleaning Our Data IV

tidy.data <- d[complete.cases(d[ , c(5, 8:10)]), ]
```

#### Exploring Our Data

Let's see how our variables relate to each other. Using `pairs.panels()` , we can produce a plot that shows us a scatter diagram of each variable in relation to each other, a histogram showing the distribution of values for each variable, and the associated correlation value for each scatter plot.

```{r}
#| label: Exploring Our Data I

pairs.panels(tidy.data[, c(5, 8:10)],
             gap = 1,
             bg = (colorRampPalette(c("blue", "red"))(161)[tidy.data$Genus]),
             pch = 21,
             hist.col = "lightpink",
             hist.border = "white")
```

This can be intimidating at first sight, but it's not as crazy as it might seem. Here it is again with the associated scatter plots and correlation values paired.

![](https://github.com/bygentry/AN588_Vignette/blob/main/Images/plot_matrix.png)

Based on these plots, it looks like brain size is strongly correlated with both male (1, r = 0.90) and female (2, r = 0.92) body mass, and male and female body mass are themselves strongly correlated (4, r = 0.97). The mass dimorphism ratio of each species doesn't seem to have a strong correlation with any of the other variables (3, r = 0.58; 5, r = 0.51; 6, r = 0.43), which is expected becuase it's a ratio between variables.

#### Data Partitioning

In order to predict the species of a primate based on these variables, we'll need to create a training data set through random sampling. Linear Discriminant Analysis requires at least one data point for each class in the training set, so let's bootstrap

We'll use 66% of the set for training and the other 34% for testing:

```{r}
#| label: Data Partitioning I

set.seed(482)

ind <- sample(2, nrow(tidy.data), 
              replace = TRUE,
              prob = c(0.66, 0.34))

training <- tidy.data[ind == 1, ]
testing <- tidy.data[ind == 2, ]

linear <- lda(Genus~., data = training)
linear
```

### Linear Discriminant Analysis

```{r}
linear <- lda(Genus~., data = training)
linear
```
